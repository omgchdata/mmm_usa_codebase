reddit_spend       = sum(spend),
reddit_view_100    = sum(watches_100_percent),
reddit_3second     = sum(three_second_views))
xx
range(xx$Week)
range(x$date)
x <- x[x$date <="2020-03-29" ,]
?file.show
list.file("//nyccentral/Annalect/BrandScience/Projects/US Army/DMA/Data/Search+Social Data/Reddit/9.3.20 Updated Weekly+DMA Data/")
list.files("//nyccentral/Annalect/BrandScience/Projects/US Army/DMA/Data/Search+Social Data/Reddit/9.3.20 Updated Weekly+DMA Data/")
xx_historical <- x %>% group_by(dma_code, dma, Week) %>%
summarise(reddit_impressions = sum(sum_impressions),
reddit_clicks      = sum(sum_clicks),
reddit_spend       = sum(spend),
reddit_view_100    = sum(watches_100_percent),
reddit_3second     = sum(three_second_views))
setwd("//nyccentral/Annalect/BrandScience/Projects/US Army/DMA/Data/Search+Social Data/Reddit/")
# reddit platform only keeps 6 months most recent data. Therefore Nick reached out to Reddit and requested "expired" data.
# This is what reddit sent on Sept 9th 2020. It is at DMA and day level
x <- read_excel("Army Weekly + DMA.xlsx")
x$date <- as.Date(x$date)
# the data ends on 03-31 (a tuesday), therefore the week of 03-30 is not a full week. We cut the data to 03-29.
x <- x[x$date <="2020-03-29",]
# let's aggregate to DMA and week level
x$Week <- as.Date(cut(x$date, breaks="week"))
xx_historical <- x %>% group_by(dma_code, dma, Week) %>%
summarise(reddit_impressions = sum(sum_impressions),
reddit_clicks      = sum(sum_clicks),
reddit_spend       = sum(spend),
reddit_view_100    = sum(watches_100_percent),
reddit_3second     = sum(three_second_views))
# now process the data Nick pulled from the reddit platform. As users can't pulled data by date/week and DMA at the same time, only in trunk of time,
# he pulled the data week by week. The next loop reads and process and combine these weekly pull of DMA data.
fl <- list.files(paste0(reddit_files, "9.3.20 Updated Weekly+DMA Data/"))
reddit_files <- "//nyccentral/Annalect/BrandScience/Projects/US Army/DMA/Data/Search+Social Data/Reddit/"
# reddit platform only keeps 6 months most recent data. Therefore Nick reached out to Reddit and requested "expired" data.
# This is what reddit sent on Sept 9th 2020. It is at DMA and day level
x <- read_excel(paste0(reddit_files, "Army Weekly + DMA.xlsx"))
x$date <- as.Date(x$date)
# the data ends on 03-31 (a tuesday), therefore the week of 03-30 is not a full week. We cut the data to 03-29.
x <- x[x$date <="2020-03-29",]
# let's aggregate to DMA and week level
x$Week <- as.Date(cut(x$date, breaks="week"))
xx_historical <- x %>% group_by(dma_code, dma, Week) %>%
summarise(reddit_impressions = sum(sum_impressions),
reddit_clicks      = sum(sum_clicks),
reddit_spend       = sum(spend),
reddit_view_100    = sum(watches_100_percent),
reddit_3second     = sum(three_second_views))
# now process the data Nick pulled from the reddit platform. As users can't pulled data by date/week and DMA at the same time, only in trunk of time,
# he pulled the data week by week. The next loop reads and process and combine these weekly pull of DMA data.
fl <- list.files(paste0(reddit_files, "9.3.20 Updated Weekly+DMA Data/"))
fl
fl[-1:-4]
i=1
# now process the data Nick pulled from the reddit platform. As users can't pulled data by date/week and DMA at the same time, only in trunk of time,
# he pulled the data week by week. The next loop reads and process and combine these weekly pull of DMA data.
fl <- list.files(paste0(reddit_files, "9.3.20 Updated Weekly+DMA Data/"))
fl <- fl[-1:-4]    # the first 4 weeks (weeks before 03-30 exist in the xx_historical, so removing them here. )
y <- read_csv(paste0(reddit_files, "9.3.20 Updated Weekly+DMA Data/", fl[i]))
head(y)
head(y)
head(y)
xx_historical$Week
seq.Date("2020-03-30", "2020-08-02", 7)
?seq.Date
seq.Date(as.Date("2020-03-30"), as.Date("2020-08-02"), 7)
head(y)
y$Week <- week[1]
week <- seq.Date(as.Date("2020-03-30"), as.Date("2020-08-02"), 7)
y$Week <- week[1]
head(y)
length(fl)
length(week)
y$`US DMA`
names(y)
names(y)[names(y) == "US DMA"]
names(y)[names(y) == "US DMA"] = "dma"
yy <- list()
week <- seq.Date(as.Date("2020-03-30"), as.Date("2020-08-02"), 7)
y <- read_csv(paste0(reddit_files, "9.3.20 Updated Weekly+DMA Data/", fl[i]))
names(y)[names(y) == "US DMA"] = "dma"
y$Week <- week[i]   # add the week column
yy[[i]] <- y %>% group_by(dma, Week) %>%
summarise(reddit_impressions = sum(Impressions),
reddit_clicks      = sum(Clicks),
reddit_spend       = sum(`Amount Spent`),
reddit_view_100    = sum(`Watches at 100%`),
reddit_3second     = sum(`3 Second Views`))
yy[[2]]
yy[[1]]
yy[[1]]$dma
xx_historical$dma
yy <- list()
week <- seq.Date(as.Date("2020-03-30"), as.Date("2020-08-02"), 7)
for (i in 1:length(fl)) {
y <- read_csv(paste0(reddit_files, "9.3.20 Updated Weekly+DMA Data/", fl[i]))
names(y)[names(y) == "US DMA"] = "dma"
y$Week <- week[i]   # add the week column
yy[[i]] <- y %>% group_by(dma, Week) %>%
summarise(reddit_impressions = sum(Impressions),
reddit_clicks      = sum(Clicks),
reddit_spend       = sum(`Amount Spent`),
reddit_view_100    = sum(`Watches at 100%`),
reddit_3second     = sum(`3 Second Views`))
}
i
length(fl)
y
fl[i]
y <- read_csv(paste0(reddit_files, "9.3.20 Updated Weekly+DMA Data/", fl[i]))
dim(y)
y
fl
fl[11]
needs(openxlsx)
needs(data.table)
#Defie the server : pc or mac
if (Sys.info()['sysname'] == "Darwin") {server <- "/Volumes"} else {server <- "//nyccentral"}
code_dir <- paste(server, "/Annalect/BrandScience/msmp/R/", sep="")
source(paste(code_dir, "Run_Model.R", sep = ""))
source(paste(code_dir, "Run_Model_Panel.R", sep = ""))
source(paste(code_dir, "my_bayes_v2.R", sep = ""))
source(paste(code_dir, "Check_Data.R", sep = ""))
source(paste(code_dir, "Transform3.R", sep = ""))
#source(paste(code_dir, "Decomp.R", sep = ""))
source(paste(code_dir, "MAPE.R", sep = ""))
source(paste(code_dir, "responsecurve.R", sep = ""))
#source(paste(code_dir, "waterfall.R", sep = ""))
#source(paste(code_dir, "DueToChart.R", sep = ""))
source(paste(code_dir, "unnestr2.0.R", sep = ""))
source(paste(code_dir, "abc.R", sep = ""))
source(paste(code_dir, "abc_onls.R", sep = ""))
source(paste(code_dir, "fitABC.R", sep = ""))
source(paste(code_dir, "unscale.R", sep = ""))
source(paste(code_dir, "Decomp_v2.R", sep = ""))
source(paste(code_dir, "act_pred.R", sep = ""))
source(paste(code_dir, "decomp_summary_panel.R", sep = ""))
source(paste(code_dir, "decomp_summary_temp.R", sep = ""))
View(Transform)
source('C:/Users/julia.liu/OneDrive - OneWorkplace/Documents/MyWork/Projects/Sovos/Noosa_Yoghurt/run_Noosa_Yoghurt.R')
names(mod_obj$data)
names(mod_obj$data)[2] = "GeoCode"
names(mod_obj$data)
names(mod_obj$data)[3] = "KPI"
names(mod_obj$data)
mod_obj$data$Intercept
mod_obj$data$Intercept = NULL
names(mod_obj$data)
excel_sheets(readxl_example("//nyccentral/Annalect/BrandScience/Projects/US Army/DMA/Data/DMA Master List Mapping_Updated.xlsx"))
excel_sheets(readxl_example("//nyccentral/Annalect/BrandScience/Projects/US Army/DMA/Data/DMA Master List Mapping_Updated.xlsx"))
excel_sheets(("//nyccentral/Annalect/BrandScience/Projects/US Army/DMA/Data/DMA Master List Mapping_Updated.xlsx"))
lkup <- read_excel("//nyccentral/Annalect/BrandScience/Projects/US Army/DMA/Data/DMA Master List Mapping_Updated.xlsx")
lkup <- read_excel("//nyccentral/Annalect/BrandScience/Projects/US Army/DMA/Data/DMA Master List Mapping_Updated.xlsx", sheet = "Reddit_DMA")
head(lkup)
table(lkup$Reddit_DMA)
t(table(lkup$Reddit_DMA))
data.frame(table(lkup$Reddit_DMA))
data.frame(table(lkup$Reddit_DMA))$Freq
unique(data.frame(table(lkup$Reddit_DMA))$Freq
)
unique(data.frame(table(lkup$Reddit_DMA))$Freq
names(lkup)
unique(data.frame(table(lkup$DMA_ID))$Freq
)
dim(lkup)
sort(lkup$Reddit_DMA)
lkup[is.na(lkup$Reddit_DMA),]
lkup[lkup$Reddit_DMA == "NA",]
lkup <- lkup[lkup$Reddit_DMA != "NA",]
dim(lkup)
head(lkup)
head(xx_historical)
names(yy[1])
names(yy[[1])
names(yy[[1]])
names(lkup)[2] = "dma_code"
head(lkup)
lkup$dma_code <- as.character(lkup$dma_code)
xx_historical2 <- full_join(lkup, xx_historical)
lkup$dma_code <- as.character(lkup$dma_code)
xx_historical2 <- full_join(lkup, xx_historical)
lkup <- read_excel("//nyccentral/Annalect/BrandScience/Projects/US Army/DMA/Data/DMA Master List Mapping_Updated.xlsx", sheet = "Reddit_DMA")
lkup <- lkup[lkup$Reddit_DMA != "NA",]
names(lkup)[2] = "dma_code"
lkup$dma_code <- as.character(lkup$dma_code)
head(xx_historical)
lkup$dma_code <- as.numeric(lkup$dma_code)
head(lkup)
xx_historical2 <- full_join(lkup, xx_historical)
dim(xx_historical)
dim(xx_historical2)
head(xx_historical)
head(xx_historical2)
nrow(xx_historical) == nrow(xx_historical2)
xx_historical2$dma = NULL   # got no use for dma column
yy[[1]]
yy = yy[[1]]
unique(yy$dma)
tmp1 <- unique(xx_historical2$Reddit_DMA)
tmp2 <- unique(yy$dma)
length(tmp1)
length(tmp2)
intersect(tmp1, tmp2)
length(intersect(tmp1, tmp2))
diff.set(tmp1, tmp2)
setdiff(tmp1, tmp2)
head(yy)
head(lkup)
names(yy)[names(yy) == "dma"] = "Reddit_DMA"
names(yy)
# attach dma code to the new data
yy2 <- full_join(lkup, yy)
dim(yy2)
dim(yy)
head(yy2)
head(xx_historical2)
# now combine historical and new data
final <- rbind(xx_historical2, yy2)
final
source('Z:/BrandScience/Projects/US Army/DMA/Data/Search+Social Data/Reddit/proc_reddit.R')
reddit_files <- "//nyccentral/Annalect/BrandScience/Projects/US Army/DMA/Data/Search+Social Data/Reddit/"
# reddit platform only keeps 6 months most recent data. Therefore Nick reached out to Reddit and requested "expired" data.
# This is what reddit sent on Sept 9th 2020. It is at DMA and day level
x <- read_excel(paste0(reddit_files, "Army Weekly + DMA.xlsx"))
x$date <- as.Date(x$date)
# the data ends on 03-31 (a tuesday), therefore the week of 03-30 is not a full week. We cut the data to 03-29.
x <- x[x$date <="2020-03-29",]
# let's aggregate to DMA and week level
x$Week <- as.Date(cut(x$date, breaks="week"))
xx_historical <- x %>% group_by(dma_code, dma, Week) %>%
summarise(reddit_impressions = sum(sum_impressions),
reddit_clicks      = sum(sum_clicks),
reddit_spend       = sum(spend),
reddit_view_100    = sum(watches_100_percent),
reddit_3second     = sum(three_second_views))
# now process the data Nick pulled from the reddit platform. As users can't pulled data by date/week and DMA at the same time, only in trunk of time,
# he pulled the data week by week. The next loop reads and process and combine these weekly pull of DMA data.
fl <- list.files(paste0(reddit_files, "9.3.20 Updated Weekly+DMA Data/"))
fl <- fl[-1:-4]    # the first 4 weeks (weeks before 03-30 exist in the xx_historical, so removing them here. )
yy <- list()
week <- seq.Date(as.Date("2020-03-30"), as.Date("2020-08-02"), 7)
for (i in 1:length(fl)) {
y <- read_csv(paste0(reddit_files, "9.3.20 Updated Weekly+DMA Data/", fl[i]))
if(nrow(y) !=0 ) {
names(y)[names(y) == "US DMA"] = "dma"
y$Week <- week[i]   # add the week column
yy[[i]] <- y %>% group_by(dma, Week) %>%
summarise(reddit_impressions = sum(Impressions),
reddit_clicks      = sum(Clicks),
reddit_spend       = sum(`Amount Spent`),
reddit_view_100    = sum(`Watches at 100%`),
reddit_3second     = sum(`3 Second Views`))
}
}
list(yy)
do.call(yy, "rbind")
dim(yy[[1]])
fl
yy[[1]]
yy[[11]]
do.call("rbind", yy)
yy <- list()
week <- seq.Date(as.Date("2020-03-30"), as.Date("2020-08-16"), 7)
for (i in 1:length(fl)) {
y <- read_csv(paste0(reddit_files, "9.3.20 Updated Weekly+DMA Data/", fl[i]))
if(nrow(y) !=0 ) {
names(y)[names(y) == "US DMA"] = "dma"
y$Week <- week[i]   # add the week column
yy[[i]] <- y %>% group_by(dma, Week) %>%
summarise(reddit_impressions = sum(Impressions),
reddit_clicks      = sum(Clicks),
reddit_spend       = sum(`Amount Spent`),
reddit_view_100    = sum(`Watches at 100%`),
reddit_3second     = sum(`3 Second Views`))
} else { # there is one week there is reddit meddia, populate that week with zeros
yy[[i]] <- yy[[1]]
yy[[i]]$Week <- week[i]
yy[[i]]$reddit_impressions = 0
yy[[i]]$reddit_clicks = 0
yy[[i]]$reddit_spend = 0
yy[[i]]$reddit_view_100 = 0
yy[[i]]$reddit_3second = 0
}
}
yy = do.call("rbind", yy)
names(yy)[names(yy) == "dma"] = "Reddit_DMA"
lkup <- read_excel("//nyccentral/Annalect/BrandScience/Projects/US Army/DMA/Data/DMA Master List Mapping_Updated.xlsx", sheet = "Reddit_DMA")
lkup <- lkup[lkup$Reddit_DMA != "NA",]
names(lkup)[2] = "dma_code"
lkup$dma_code <- as.numeric(lkup$dma_code)
xx_historical2 <- full_join(lkup, xx_historical)
nrow(xx_historical) == nrow(xx_historical2)
xx_historical2$dma = NULL   # got no use for dma column
tmp1 <- unique(xx_historical2$Reddit_DMA)
tmp2 <- unique(yy$dma)
setdiff(tmp1, tmp2)
# attach dma code to the new data
yy2 <- full_join(lkup, yy)
nrow(yy)
nrow(yy2)
# now combine historical and new data
final <- rbind(xx_historical2, yy2)
final[1,]
dim(final)
dim(yy2)
dim(xx_historical2)
head(final)
write_csv(final, paste0(reddit_files, "reddited_dma_week_processed.csv" ))
source('C:/Users/julia.liu/OneDrive - OneWorkplace/Documents/MyWork/msmp/doc/msmp_panel_example/run_msmp_panel_example.R')
source('C:/Users/julia.liu/OneDrive - OneWorkplace/Documents/MyWork/msmp/doc/msmp_panel_example/run_msmp_panel_example.R')
names(mod_obj$data)
unique((mod_obj$data$GeoCode)
)
dim(mod_obj$data)
head(mod_obj$data)
mod_obj$data$KPI
library(car)
library(lmtest)
library(tidyverse)
library(lubridate)
needs(openxlsx)
needs(data.table)
#Defie the server : pc or mac
if (Sys.info()['sysname'] == "Darwin") {server <- "/Volumes"} else {server <- "//nyccentral"}
code_dir <- paste(server, "/Annalect/BrandScience/msmp/R/", sep="")
source(paste(code_dir, "Run_Model.R", sep = ""))
source(paste(code_dir, "Run_Model_Panel.R", sep = ""))
source(paste(code_dir, "my_bayes_v2.R", sep = ""))
source(paste(code_dir, "Check_Data.R", sep = ""))
source(paste(code_dir, "Transform3.R", sep = ""))
#source(paste(code_dir, "Decomp.R", sep = ""))
source(paste(code_dir, "MAPE.R", sep = ""))
source(paste(code_dir, "responsecurve.R", sep = ""))
#source(paste(code_dir, "waterfall.R", sep = ""))
#source(paste(code_dir, "DueToChart.R", sep = ""))
source(paste(code_dir, "unnestr2.0.R", sep = ""))
source(paste(code_dir, "abc.R", sep = ""))
source(paste(code_dir, "abc_onls.R", sep = ""))
source(paste(code_dir, "fitABC.R", sep = ""))
source(paste(code_dir, "unscale.R", sep = ""))
source(paste(code_dir, "Decomp_v2.R", sep = ""))
source(paste(code_dir, "act_pred.R", sep = ""))
source(paste(code_dir, "decomp_summary_panel.R", sep = ""))
source(paste(code_dir, "decomp_summary_temp.R", sep = ""))
#######  define project directories ##############
# please edit these lines to define the path to the project folder.
ProjectName <-  "msmp_panel_example"            # the name of the subfolder that contains the model project
OutDir <- "output"
RootDirectory <- "C:/Users/julia.liu/OneDrive - OneWorkplace/Documents/MyWork/msmp/doc/"
ProjectDirectory <- paste(RootDirectory, ProjectName, "/", sep="")   # this is the full path of the project
###################
# setup
###################
# define input file names
ModelDataFile <- paste(ProjectDirectory, ProjectName, "_ModelData.csv", sep="")
ModelSetupFile <- paste(ProjectDirectory, ProjectName, "_ModelSetup.csv", sep="")
ModelSpecFile <- paste(ProjectDirectory, ProjectName, "_Variables.csv", sep="")
# output file names
output_folder <- paste(ProjectDirectory, OutDir, sep="")
if(!file.exists(output_folder)) {
dir.create(output_folder)
}
PriorFile <- paste(output_folder,  "/", ProjectName, "_Priors.csv", sep="")
CoefficientsFile <- paste(output_folder,  "/", ProjectName, "_Coefficients.csv", sep="")
ActualPredictedFile <- paste(output_folder, "/", ProjectName, "_ActPred.csv", sep="")
DecompFile <- paste(output_folder,  "/", ProjectName, "_Decomp.csv", sep="")
spec_coef_vifFile <- paste(output_folder,  "/", ProjectName, "_spec_coef_vif.csv", sep="")
RCFile <- paste(output_folder,"/", ProjectName, "_ResponseCurve.csv", sep="")
kpi_spentFile <- paste(output_folder,"/", ProjectName, "_kpi_spent.csv", sep="")
ModObjectFile <- paste(output_folder, "/", ProjectName, "_ModObj.RData", sep="")
# read input files
x <- read_csv(ModelDataFile, col_types = cols())
x$Week<- mdy(x$Week)
# read input files
x <- read_csv(ModelDataFile, col_types = cols())
names(x)
source('C:/Users/julia.liu/OneDrive - OneWorkplace/Documents/MyWork/msmp/doc/msmp_panel_example/run_msmp_panel_example.R')
names(mod_obj$data)
x
x$KPI = x$KPI/1000
unique(x$GeoCode)
x$GeoCode[x$GeoCode == "DMA100000"]
x$GeoCode[x$GeoCode == "DMA100000"] = "geo1"
x$GeoCode[x$GeoCode == "DMA501"] = "geo2"
x$GeoCode[x$GeoCode == "DMA506"] = "geo3"
x$GeoCode[x$GeoCode == "DMA512"] = "geo4"
x$GeoCode[x$GeoCode == "DMA535"] = "geo5"
x$GeoCode[x$GeoCode == "DMA573"] = "geo6"
x$GeoCode[x$GeoCode == "DMA602"] = "geo7"
x$GeoCode[x$GeoCode == "DMA659"] = "geo8"
x$GeoCode[x$GeoCode == "DMA807"] = "geo9"
x$GeoCode[x$GeoCode == "DMA819"] = "geo10"
head(x)
unique(x$GeoCode)
table(x$GeoCode)
x = x[order(x$GeoCode, x$Week)]
x = x[order(x$GeoCode, x$Week),]
x
dim(x)
table(x$Week)
table(x$GeoCode)
head(x)
x$search
x$search = x$search*100000
x$search
View(x)
getwd()
setwd("C:/Users/julia.liu/OneDrive - OneWorkplace/Documents/MyWork/msmp/doc/msmp_panel_example")
write_csv(x, "msmp_panel_example_ModelData.csv")
filter
?filter
source('C:/Users/julia.liu/OneDrive - OneWorkplace/Documents/MyWork/msmp/doc/msmp_panel_example/run_msmp_panel_example.R')
source('C:/Users/julia.liu/OneDrive - OneWorkplace/Documents/MyWork/msmp/doc/msmp_panel_example/run_msmp_panel_example.R')
source('C:/Users/julia.liu/OneDrive - OneWorkplace/Documents/MyWork/msmp/doc/msmp_panel_example/run_msmp_panel_example.R')
mod_obj$spec <- read_csv(ModelSpecFile, col_types=cols()) %>%
dplyr::filter(Include == 1) # use Include to include (1)/exclude (0) variables
ProjectName <-  "msmp_panel_example"            # the name of the subfolder that contains the model project
OutDir <- "output"
RootDirectory <- "C:/Users/julia.liu/OneDrive - OneWorkplace/Documents/MyWork/msmp/doc/"
ProjectDirectory <- paste(RootDirectory, ProjectName, "/", sep="")   # this is the full path of the project
###################
# setup
###################
# define input file names
ModelDataFile <- paste(ProjectDirectory, ProjectName, "_ModelData.csv", sep="")
ModelSetupFile <- paste(ProjectDirectory, ProjectName, "_ModelSetup.csv", sep="")
ModelSpecFile <- paste(ProjectDirectory, ProjectName, "_Variables.csv", sep="")
# output file names
output_folder <- paste(ProjectDirectory, OutDir, sep="")
if(!file.exists(output_folder)) {
dir.create(output_folder)
}
PriorFile <- paste(output_folder,  "/", ProjectName, "_Priors.csv", sep="")
CoefficientsFile <- paste(output_folder,  "/", ProjectName, "_Coefficients.csv", sep="")
ActualPredictedFile <- paste(output_folder, "/", ProjectName, "_ActPred.csv", sep="")
DecompFile <- paste(output_folder,  "/", ProjectName, "_Decomp.csv", sep="")
spec_coef_vifFile <- paste(output_folder,  "/", ProjectName, "_spec_coef_vif.csv", sep="")
RCFile <- paste(output_folder,"/", ProjectName, "_ResponseCurve.csv", sep="")
kpi_spentFile <- paste(output_folder,"/", ProjectName, "_kpi_spent.csv", sep="")
ModObjectFile <- paste(output_folder, "/", ProjectName, "_ModObj.RData", sep="")
#DecompSumFile <- paste(output_folder, "/", ProjectName, "_DecompSummary.csv",  sep="")
# read input files
x <- read_csv(ModelDataFile, col_types = cols())
#x$Week<- mdy(x$Week)
# you can add some variables that are not in the _ModelData.csv here.
# For example dummy variable and etc.
mod_obj <- list()
mod_obj$data <- x
mod_obj$spec <- read_csv(ModelSpecFile, col_types=cols()) %>%
dplyr::filter(Include == 1) # use Include to include (1)/exclude (0) variables
mod_obj$setup <- read_csv(ModelSetupFile, col_types = cols())
mod_obj <- msmp_setup(mod_obj)
#######################################
# data check
#######################################
mod_obj <- Check_Data(mod_obj)
#########################
# variable transformation
#########################
mod_obj <- Transform_panel(mod_obj)
mod_obj$data <- mod_obj$data[mod_obj$data[[mod_obj$Time]] >= mod_obj$BeginDate & mod_obj$data[[mod_obj$Time]] <= mod_obj$EndDate,]
########################
# Run model
########################
print("Run model...")
#mod_obj <- Run_Model(obj = mod_obj)
mod_obj <- Run_Model_Panel(mod_obj)
write_csv(mod_obj$Model$Priors, PriorFile)
write_csv(mod_obj$Model$coefficients, CoefficientsFile)
write_csv(mod_obj$Model$act_pred, ActualPredictedFile)
mod_obj <- Decomp(obj = mod_obj, incl_spent = F)
write_csv(mod_obj$Decomposition_panel, DecompFile)
decomp_sum <- panel_decomp(mod_obj)
#decomp_sum <- panel_decomp(mod_obj)
#write_csv(decomp_sum, DecompSumFile)
####################
# Simulation to get response curve
####################
mod_obj <- responsecurve_panel(obj = mod_obj, showPlot=FALSE )
mod_obj$setup$Parameter
mod_obj$setup
source('C:/Users/julia.liu/OneDrive - OneWorkplace/Documents/MyWork/msmp/doc/msmp_panel_example/run_msmp_panel_example.R')
mod_obj <- Run_Model(obj = mod_obj)
mod_obj$SimEnd
mod_obj <- msmp_setup(mod_obj)
mod_obj$BeginDate
mod_obj$EndDate
# define model object
mod_obj <- list()
mod_obj$data <- x
mod_obj$spec <- read_csv(ModelSpecFile, col_types=cols()) %>%
dplyr::filter(Include == 1) # use Include to include (1)/exclude (0) variables
mod_obj$setup <- read_csv(ModelSetupFile, col_types = cols())
obj = mod_obj
Model_setup <- obj$setup
Model_setup
as.numeric(Model_setup$Value[Model_setup$Parameter == "Mroi"])
source('C:/Users/julia.liu/OneDrive - OneWorkplace/Documents/MyWork/msmp/doc/msmp_panel_example/run_msmp_panel_example.R')
mod_obj$Time
mod_obj
obj  = mod_obj
Model_setup <- obj$setup
obj$ModelForm <- Model_setup$Value[tolower(Model_setup$Parameter=="modelform")]
obj$Panel <- Model_setup$Value[tolower(Model_setup$Parameter=="panel")]
obj$Panel
Model_setup$Value[tolower(Model_setup$Parameter=="panel")]
tolower(Model_setup$Parameter=="panel")
tolower(Model_setup$Parameter)=="panel"
source('C:/Users/julia.liu/OneDrive - OneWorkplace/Documents/MyWork/msmp/doc/msmp_panel_example/run_msmp_panel_example.R')
source('C:/Users/julia.liu/OneDrive - OneWorkplace/Documents/MyWork/msmp/doc/msmp_panel_example/run_msmp_panel_example.R')
q()
